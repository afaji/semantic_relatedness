{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stopwords, lowercase and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(stopwords_file):\n",
    "    stopwords_df = pd.read_csv(stopwords_file)\n",
    "    stopwords = set(stopwords_df['stopwords'].values)\n",
    "    return stopwords\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Lowercase the sentence\n",
    "    sentence = sentence.lower()\n",
    "    # Remove any special characters\n",
    "    sentence = ''.join(c for c in sentence if ord(c) < 128)\n",
    "    return sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find lexical overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lexical_overlap(text_file, stopwords_file):\n",
    "    # Read the text file\n",
    "    with open(text_file, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "\n",
    "    # Load stopwords from CSV file\n",
    "    stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "    # List to store sentence pairs with lexical overlap\n",
    "    sentence_pairs = []\n",
    "\n",
    "    # Dictionary to store sentence occurrence count\n",
    "    sentence_counts = defaultdict(int)\n",
    "\n",
    "    # Iterate over sentences to find pairs with lexical overlap\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = clean_sentence(sentences[i])\n",
    "        # Check if sentence length is between 5 and 25 words\n",
    "        if 5 <= len(sentences[i].split()) <= 25:\n",
    "            for j in range(i+1, len(sentences)):\n",
    "                sentences[j] = clean_sentence(sentences[j])\n",
    "                # Check if sentence length is between 5 and 25 words\n",
    "                if 5 <= len(sentences[j].split()) <= 25:\n",
    "                    words1 = set(sentences[i].split())\n",
    "                    words2 = set(sentences[j].split())\n",
    "                    # Remove stopwords from words\n",
    "                    words1 = words1.difference(stopwords)\n",
    "                    words2 = words2.difference(stopwords)\n",
    "                    overlap = words1.intersection(words2)\n",
    "                    if len(overlap) >= 5:  # Choose the lexical overlap\n",
    "                        # Only add pair if neither sentence has appeared more than twice\n",
    "                        if sentence_counts[sentences[i]] < 2 and sentence_counts[sentences[j]] < 2:\n",
    "                            sentence_pairs.append((sentences[i], sentences[j]))\n",
    "                            # Increase count for each sentence\n",
    "                            sentence_counts[sentences[i]] += 1\n",
    "                            sentence_counts[sentences[j]] += 1\n",
    "\n",
    "    df = pd.DataFrame(sentence_pairs, columns=[\"Sentence 1\", \"Sentence 2\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence 1</th>\n",
       "      <th>Sentence 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mun auna cewa za a samu kyakkyawan shugabanci ...</td>\n",
       "      <td>don haka masu oarin ata mana suna har su na ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>makonni biyu da suka gabata, wannan jarida ta ...</td>\n",
       "      <td>makonni biyu ne wannan jarida ta buga labarin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jamiyyun sun haa da pdp, lp, nnpp, apga, sdp, ...</td>\n",
       "      <td>jamiyyun da su ka karya wannan doka sun haa ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tuni dai su ka kafa kwamitin mambobi 13 arashi...</td>\n",
       "      <td>gwamnan jihar delta kuma an takarar mataimakin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tuni dai su ka kafa kwamitin mambobi 13 arashi...</td>\n",
       "      <td>wike da wasu gwamnonin pdp huu dai su na so sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence 1   \n",
       "0  mun auna cewa za a samu kyakkyawan shugabanci ...  \\\n",
       "1  makonni biyu da suka gabata, wannan jarida ta ...   \n",
       "2  jamiyyun sun haa da pdp, lp, nnpp, apga, sdp, ...   \n",
       "3  tuni dai su ka kafa kwamitin mambobi 13 arashi...   \n",
       "4  tuni dai su ka kafa kwamitin mambobi 13 arashi...   \n",
       "\n",
       "                                          Sentence 2  \n",
       "0  don haka masu oarin ata mana suna har su na ce...  \n",
       "1  makonni biyu ne wannan jarida ta buga labarin ...  \n",
       "2  jamiyyun da su ka karya wannan doka sun haa ha...  \n",
       "3  gwamnan jihar delta kuma an takarar mataimakin...  \n",
       "4  wike da wasu gwamnonin pdp huu dai su na so sh...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = \"./sentence.txt\"  # \n",
    "stopwords_file = './stopwords_hausa.csv'  # \n",
    "result_df_hausa = find_lexical_overlap(text_file, stopwords_file)\n",
    "\n",
    "result_df_hausa.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in result_df_hausa.iterrows():\n",
    "    print(\"S1:\", row[\"Sentence 1\"])\n",
    "    print(\"S2:\", row[\"Sentence 2\"])\n",
    "    print(\"----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 scripts/semantic_relatedness.py \\\n",
    "  -i data/sentence.txt \\\n",
    "  -s data/stopwords_hausa.csv \\\n",
    "  -o results \\\n",
    "  --clean_sentences \\\n",
    "  --remove_stopwords \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
